---
title: "A/B testing, Bayes style"
output:  html_document
---

```{r, echo=FALSE}
source('bayesianABTesting.R')
#theme_set(theme_minimal(12))
```

##But why?
So that you understand your test results correctly, including the uncertainty in them, without the p-value hassle.

###Also:

Gentle introduction to Bayesian methods.

###After this
You understand the results of Bayesin analysis, and are familiar with terms such as _posterior_,  _prior_, and _statistical model_.

***

##Our case

Reaktor website has a one-click purchase of a fixed-price-scope schedule-team agile software project, or whatever. More to the point, there are _two variants of the button_. Which is better? How much?

**What do we measure**? 

Visits on the page and button clicks.

Below are some simulated data, with pre-set click rates. Those are the underlying rates that we do not know in the measurement situation, and which we are after. Per day, we have on average 1500 visits on the page, of which 20% see the option __B__, for a total of six days.

Note that if you re-knit this document from the R Markdown file, data is re-generated and the results will change. How clear the difference between __A__ and __B__ looks like, from the raw click rates, _varies by run_. _That's also how it is in real life!_ Our goal is to get beyond that variation to get an idea of how things are in reality. 

Reality here refers to the pre-set click rates. In practice we also have pre-set click rates, but they are set by  nature. We do not observe them directly. We observe only data.

```{r}
rate.A <- 0.052
rate.B <- 0.068

example <- click.data(rate.A, rate.B, mean.visits=1500, test.group=0.2, days=6)

print(example)
```

So this is the (simulated) setup, and the data above is all we know from measurements. 

---

That's how the data looks like as plots:

```{r}
example.plots <- click.plots(example)
print(example.plots$histograms)
print(example.plots$rates)
```

---

##Let's mathematics 

(If you want results without too much understanding, skip to the practical section below.)

_What do we want to know?_ The pre-defined click rates, set by nature,  $\theta_A$ and $\theta_B$ that the visitor buys a project when shown either button __A__ or __B__. In our example where we played nature and generated the data, the rates were __rate.A__ and __rate.B__.

_What do we have?_ Click and visit counts, $k$ and $N$.

---

###A statistical model

Think how the data is generated.

If the click rate or the change to click is $\theta$ per visit, the probability to get a click/no-click sequence 00101 is

$$(1-\theta)\,(1-\theta)\,\theta\,(1-\theta)\,\theta\;.$$

For a _single_ sequence of $k$ clicks and $N-k$ non-clicks, the probability is

$$\theta^k\, (1-\theta)^{N-k}\;.$$

The probability to get _any_ sequence with $k$ clicks and $N-k$  non-cliks is

$$\text{(Number of sequences with $k$ clicks and $N-k$ non-clicks)} \times \theta^k\, (1-\theta)^{N-k}\;.$$

That number of sequences does not depend on click rate, so we don't need to care about it really. But for completeness, it is $N!/(k!(N-k)!)$. So the probability to get a data, or in other words any sequence, with $k$ clicks is

$$p(k, N|\theta) = \frac{N!}{k!(N-k)!} \theta^k\, (1-\theta)^{N-k}\, \propto\, \theta^k\, (1-\theta)^{N-k}\;.$$

That "$\propto$" is "proportional to", and just means that we eventually can skip the constant (as we will see). (Also note that the probability is in no way absolute, the same in all settings, but conditional on the click rate $\theta$.)

Okey, so we know the probability of data of length $N$ with $k$ clicks, given click rate $\theta$. Sadly, this is the wrong way, as it always tends to be statistics.

Now we have the probability of data we have seen, given reality. But that reality is unknown! What we want is the probability of reality given the data se have.

---

###Enter Bayes

Think about the conditional probability for a while. What is the probability to see a full moon tonight? It is, to a first approximation, the probability that it is full-moon time in general, times the probability that you see the moon, if it is there:

$$p(\text{full moon & seen}) = p(\text{seen}\, |\, \text{full moon}) \times p(\text{full moon})$$. 

You can invent better examples, but the general idea is that for two events $X$ and $Y$, the probability of both occurring is the probability that the first ($X$) occurs, times the probability that $Y$ occurs given $X$, or vice versa.

$$p(X, Y) = p(X | Y)\, p(Y) = p(Y | X)\, p(X)\;.$$

This is quite self-evident, but powerful when applied in a more abstract context. What is the probability of seeing data $D$ and being in world $W$?

$$p(D, W) = p(D | W)\, p(W) = p(W | D)\, p(D)\;.$$

In other words, 

$$
p(W | D) = \frac{p(D | W)\, p(W)}{p(D)}\;.
$$

This is quite remarkable, called _the Bayes rule_. It says that if you have _a model_ that gives the probability of data $D$, given a state of the world $W$, you can get the probability of _world itself, given data_, by simple multiplication (or so it seems).

How does this apply to our click rates? Our data $D$ are the observed $k, N$, and our world is described by one simple probability, the click rate $\theta$. Applying Bayes rule, we have
$$
p(\theta | k, N) = \frac{p(k, N\, | \, \theta)\, p(\theta)}{p(k, N)} 
= \frac{N!}{k!(N-k)!} \theta^k\, (1-\theta)^{N-k}\; \frac{p(\theta)}{p(k, N)}\;.
$$

At this point it helps a lot to note that in a practical setting the data $N$ and $k$ are what they are, they do not change. If we are after $p(\theta\,|\,N, k)$, and especially willing to mainly know how likely different click rates $\theta$ are _compared to each other_, we can drop all constant multiplies. That is, we can drop all multipliers where $\theta$ does not appear. This includes the weird "probability of data" $p(k, N)$ which is constant anyway, as well as the binomial coefficients $N!/(k!(N-k)!)$ originally arising from the number of possible click sequences. We are then left with just

$$ 
p(\theta\, |\, k, N) \propto \theta^k\, (1-\theta)^{N-k}\;p(\theta)\;.
$$
The constant left out form our "propto" or " $\propto$ can be later determined, if needed, from the fact that the sum or integral of the probabilities need to be one: $\int p(\theta\,|\,k, N)\;d\theta$ = 1. (If you google for "beta distribution", you'll find the constant.)

The probability distribution $p(\theta\, |\, k, N)$ is called _the posterior_, because it carries our information of the world _after_ the data.

There is one more thing: $p(\theta)$, or the probability of various click rates before any data. WTF is that? It is my prejudice or domain knowledge about how the click rates in general are, prior to seeing data. If I have no idea, a decent alternative is to set $p(\theta)=\text{const.}$, or that all click rates are equally good guesses. ($p(\theta)$ is called _prior_, and we'll come back to it later.)

---

Ok, back to reality. :) 
How do $p(\theta\,|\,N, k)$ look like in practice, for the counts of clicks and visits we have? Below are plots for daily click rates. There is now a separate click rate $\theta$ behind the observed clicks of each day, and for variants __A__ and __B__ separately. That is, 12 different $\theta$'s. (We will stick with two later.) Dashed lines are the true values, that we happen to (exceptionally) know because we're simulating nature here...


```{r}
example.posterior <- abtest.posteriorplot(example)
# quartz(width=14, height=6)
print(example.posterior$plot
      + geom_vline(xintercept=rate.A, linetype='dashed', colour="dodgerblue4")
      + geom_vline(xintercept=rate.B, linetype='dashed', colour="goldenrod2")
      + scale_color_manual(values=c("dodgerblue4", "goldenrod2"))
      + scale_y_continuous(breaks=NULL)
      + scale_x_continuous(breaks=c(.02, .05, .1))
      + xlab("theta") + ylab("probability (density)"))

```

## Getting practical

Happened so far: Given number of clicks $k$, and the total number of visits, $N$, we were able to compute the _probability distribution_ of the underlying true click rate, given the data:
$$
p(\theta\, |\, k, N) \propto \theta^k\, (1-\theta)^{N-k}\.
$$

Probability distributions of things in the world are called _posteriors_ in general, because they come after data.  This particular posterior is of the mathematical form called _Beta distribution_. Not all probabilities of unknown things in the world are Beta distributed. Here it arises because of our model happens to be such. 

### Simulating from a posterior

If you google for Beta distribution, you may stumble functions that _generate "random variates" that are Beta distributed_. A random variate here is a fancy word for a number. :) In R the relevant function is __rbeta()__. 

So you have 212 visits in total, and 13 clicks, and do want to see possible values for the true click rate?
```{r, message=FALSE, warning=F}
click.rate.samples <- rbeta(10000, 13+1, 212-13+1)
qplot(click.rate.samples) + geom_histogram() + xlim(0, .2)
```

So that was quite easy. :) Those $+1$'s come from the $p(\theta)$ of the previous section, and they basically say "Prior to data, I have no idea what the click rate is, or even if I have, I don't want to tell.". 

Compare to the Beta distribution obtained with our function
````{r, message=FALSE, warning=F}
betaplots(mean=13/212, sample.size=212) + xlim(0, .2)
```

In practice, managing our result $p(\theta\,|\,N, k)$ is often easier if we take those random samples from __rbeta()__, instead of trying to track the analytical distribution. There are analogous functions in Python or Javascript for example. 

For many simple models, the posteriorcan be written as a formula, or "in analytical form" as statisticians say. If there are a lot of data, it can often be approximated by the normal distribution even if the model is not so simple. Then there are models for which we cannot have a analytic posterior, not even a good approximation. But maybe surprisingly, we can often still get samples from the posterior! This may require heavy computation.

Here we stick with our particular simple model, and __rbeta()__ is sufficient.


### How about the comparison of A and B?

Samples of click rate $\theta$ are possible states of the world. We can pair those up for treatments A and B to see how the difference is in possible worlds, given the data.

```{r, message=FALSE}
click.rate.samples <- rbeta(100000, 13 +1, 212-13 +1) - rbeta(100000, 20 +1, 400-20 +1)
qplot(click.rate.samples) + geom_histogram() + xlim(-.10, .10)
```

What is the probability for the $(212, 13)$ case to have a larger click rate?
```{r}
sum(click.rate.samples>0)/length(click.rate.samples) # Or simply mean(...)
````
So click rate looks like larger in the first case, but we are far from sure. 

Let's look our example data again.

```{r, warning=F, message=F}
example
visits.a <- sum(example[example$variant=="A", "visits"])
visits.b <- sum(example[example$variant=="B", "visits"])
clicks.a <- sum(example[example$variant=="A", "clicks"])
clicks.b <- sum(example[example$variant=="B", "clicks"])

c(visits.a, visits.b, clicks.a, clicks.b)
rate.a <- rbeta(100000, 1 + clicks.a, 1 + visits.a - clicks.a)
rate.b <- rbeta(100000, 1 + clicks.b, 1 + visits.b - clicks.b)

c(mean(rate.a), mean(rate.b), mean(rate.a - rate.b))
mean(rate.a > rate.b) # Probability of rate.a being larger
qplot(rate.a-rate.b) + geom_histogram() 

````


### "I have no prior idea of the click rate" ... what if you have?

Conjugate priors

### What happens to the posterior when data increases?

It gets narrower, and starts to resemble the gaussian (normal) distribution. The influence of prior vanishes.

```{r, warning=F}
betaplots(mean=0.02) + xlim(0, .2) + xlab("theta (click rate)")
```

### What is the classic A/B testing in this context? How about p-values?

Frequentist, classic significance test for the click rates assumes gaussian posteriors, and ignores the prior. The shape of the gaussian approximation can be quite easily computed from data. The problem then turns to comparing two normal distributions, and the concept of z-score appears. 

Frequentist p-values are probabilities to get the difference you see, or larger, if there is no real difference. Our probabilities for a difference of the click rates to be larger or smaller than zero are identical to frequentist p-values if (1) you have a lot of data; (2) click rate is not too small; (3) the frequentist p-value is "one-sided", not "two-sided"; (4) prior is non-informative.

### Extensions to classic A/B testing

Multiple treatments, not just A and B. 

Using covariates: the viewers have one or more known properties, and you want to see how they affect the click rate, or difference of click rates between groups. 

Analyse temporal effects

Hierarchy for groups: A1, A2, A3, A4, B1, B2, B3, B4. You may find differences between A and B even if subgroup differences are not significant. 

Cross designs: Alternate two or more variables simultaneously. 

Continuous treatments: Instead of discrete groups, have a thing that varies continuously, such as a button size or color. 

Incremental (sequential) experiments: get results when you have enough data, but don't compromise the results by "waiting for significance". 

Automatic optimization: the system experiments all the time, and directs resources where posterior uncertainty is large and utilities are high. 

### What you need to remember

Prior are your preconceptions.

Posterior is what is known after data.

Statistical model tells how we see the world in our experiment. 

__rbeta()__ or an equivalent. Samples are possible states of the world.




### Even more important things

We assume __unbiased samples__, and independent clicks.

Treatment effects vary by time, and results of a quick A/B test may be misleading.